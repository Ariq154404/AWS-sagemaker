{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21503c58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:06.002906Z",
     "iopub.status.busy": "2022-09-27T14:16:06.000392Z",
     "iopub.status.idle": "2022-09-27T14:16:26.936491Z",
     "shell.execute_reply": "2022-09-27T14:16:26.937000Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import pickle\n",
    "import io\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c01395",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:26.942242Z",
     "iopub.status.busy": "2022-09-27T14:16:26.941423Z",
     "iopub.status.idle": "2022-09-27T14:16:26.945094Z",
     "shell.execute_reply": "2022-09-27T14:16:26.944549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12428bc6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:26.949172Z",
     "iopub.status.busy": "2022-09-27T14:16:26.948532Z",
     "iopub.status.idle": "2022-09-27T14:16:26.951175Z",
     "shell.execute_reply": "2022-09-27T14:16:26.950721Z"
    }
   },
   "outputs": [],
   "source": [
    "context_length=5\n",
    "prediction_length=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c1036f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:26.955056Z",
     "iopub.status.busy": "2022-09-27T14:16:26.954476Z",
     "iopub.status.idle": "2022-09-27T14:16:26.957358Z",
     "shell.execute_reply": "2022-09-27T14:16:26.956814Z"
    }
   },
   "outputs": [],
   "source": [
    "freq=\"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10051e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:26.969872Z",
     "iopub.status.busy": "2022-09-27T14:16:26.968990Z",
     "iopub.status.idle": "2022-09-27T14:16:27.294675Z",
     "shell.execute_reply": "2022-09-27T14:16:27.294149Z"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c821835",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:27.308005Z",
     "iopub.status.busy": "2022-09-27T14:16:27.307057Z",
     "iopub.status.idle": "2022-09-27T14:16:27.504116Z",
     "shell.execute_reply": "2022-09-27T14:16:27.503500Z"
    }
   },
   "outputs": [],
   "source": [
    "prefix = \"bit-coin-forecast\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "bucket = \"bit-coin-data\"\n",
    "\n",
    "s3_data_path = f\"{bucket}/{prefix}/data\"\n",
    "s3_output_path = f\"{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff980a91",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:27.510030Z",
     "iopub.status.busy": "2022-09-27T14:16:27.509264Z",
     "iopub.status.idle": "2022-09-27T14:16:27.511514Z",
     "shell.execute_reply": "2022-09-27T14:16:27.511958Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def downloadDirectoryFroms3(bucketName, remoteDirectoryName):\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    bucket = s3_resource.Bucket(bucketName) \n",
    "    for obj in bucket.objects.filter(Prefix = remoteDirectoryName):\n",
    "        if not os.path.exists(os.path.dirname(obj.key)):\n",
    "            os.makedirs(os.path.dirname(obj.key))\n",
    "        bucket.download_file(obj.key, obj.key) # save to same path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19d84801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:27.523735Z",
     "iopub.status.busy": "2022-09-27T14:16:27.523146Z",
     "iopub.status.idle": "2022-09-27T14:16:43.785496Z",
     "shell.execute_reply": "2022-09-27T14:16:43.784941Z"
    }
   },
   "outputs": [],
   "source": [
    "downloadDirectoryFroms3(\"bit-coin-data\", \"bit-coin-forecast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e38f211f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:43.789739Z",
     "iopub.status.busy": "2022-09-27T14:16:43.789173Z",
     "iopub.status.idle": "2022-09-27T14:16:43.798525Z",
     "shell.execute_reply": "2022-09-27T14:16:43.798008Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('bit-coin-forecast/data/output/ground_truth.pkl', 'rb') as file:\n",
    "    time_series= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0991bf7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:43.803291Z",
     "iopub.status.busy": "2022-09-27T14:16:43.802405Z",
     "iopub.status.idle": "2022-09-27T14:16:43.811700Z",
     "shell.execute_reply": "2022-09-27T14:16:43.811209Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('bit-coin-forecast/data/output/train.pkl', 'rb') as file:\n",
    "     time_series_training= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a47a0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:43.816367Z",
     "iopub.status.busy": "2022-09-27T14:16:43.815479Z",
     "iopub.status.idle": "2022-09-27T14:16:43.819175Z",
     "shell.execute_reply": "2022-09-27T14:16:43.818597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(time_series[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a3a9df8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:43.824576Z",
     "iopub.status.busy": "2022-09-27T14:16:43.824010Z",
     "iopub.status.idle": "2022-09-27T14:16:43.827034Z",
     "shell.execute_reply": "2022-09-27T14:16:43.826466Z"
    }
   },
   "outputs": [],
   "source": [
    "def series_to_obj(ts, cat=None):\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": list(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    return obj\n",
    "\n",
    "\n",
    "def series_to_jsonline(ts, cat=None):\n",
    "    return json.dumps(series_to_obj(ts, cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2e61bfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:43.834702Z",
     "iopub.status.busy": "2022-09-27T14:16:43.832599Z",
     "iopub.status.idle": "2022-09-27T14:16:44.449142Z",
     "shell.execute_reply": "2022-09-27T14:16:44.448523Z"
    }
   },
   "outputs": [],
   "source": [
    "encoding = \"utf-8\"\n",
    "FILE_TRAIN = \"train.json\"\n",
    "FILE_TEST = \"test.json\"\n",
    "with open(FILE_TRAIN, \"wb\") as f:\n",
    "    for ts in time_series_training:\n",
    "        f.write(series_to_jsonline(ts).encode(encoding))\n",
    "        f.write(\"\\n\".encode(encoding))\n",
    "\n",
    "with open(FILE_TEST, \"wb\") as f:\n",
    "    for ts in time_series:\n",
    "        f.write(series_to_jsonline(ts).encode(encoding))\n",
    "        f.write(\"\\n\".encode(encoding))\n",
    "s3 = boto3.client(\"s3\")\n",
    "s3.upload_file(FILE_TRAIN, bucket, prefix + \"/data/train/\" + FILE_TRAIN)\n",
    "s3.upload_file(FILE_TEST, bucket, prefix + \"/data/test/\" + FILE_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b4e4a0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:44.462658Z",
     "iopub.status.busy": "2022-09-27T14:16:44.457388Z",
     "iopub.status.idle": "2022-09-27T14:16:44.513315Z",
     "shell.execute_reply": "2022-09-27T14:16:44.512640Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The method get_image_uri has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image_uri = get_image_uri(boto3.Session().region_name, \"forecasting-deepar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5d29edd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:44.518104Z",
     "iopub.status.busy": "2022-09-27T14:16:44.517515Z",
     "iopub.status.idle": "2022-09-27T14:16:44.519896Z",
     "shell.execute_reply": "2022-09-27T14:16:44.520381Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=f\"s3://{s3_output_path}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d2e7074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:44.525195Z",
     "iopub.status.busy": "2022-09-27T14:16:44.524546Z",
     "iopub.status.idle": "2022-09-27T14:16:44.527498Z",
     "shell.execute_reply": "2022-09-27T14:16:44.527018Z"
    }
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"num_cells\": \"40\",\n",
    "    \"num_layers\": \"3\",\n",
    "    \"likelihood\": \"gaussian\",\n",
    "    \"epochs\": \"20\",\n",
    "    \"mini_batch_size\": \"32\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"dropout_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f632aa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:44.531903Z",
     "iopub.status.busy": "2022-09-27T14:16:44.531310Z",
     "iopub.status.idle": "2022-09-27T14:16:44.534207Z",
     "shell.execute_reply": "2022-09-27T14:16:44.533663Z"
    }
   },
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b21d9af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:16:44.539827Z",
     "iopub.status.busy": "2022-09-27T14:16:44.538837Z",
     "iopub.status.idle": "2022-09-27T14:21:28.809455Z",
     "shell.execute_reply": "2022-09-27T14:21:28.808983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-27 14:16:44 Starting - Starting the training job...\n",
      "2022-09-27 14:17:07 Starting - Preparing the instances for trainingProfilerReport-1664288204: InProgress\n",
      ".........\n",
      "2022-09-27 14:18:44 Downloading - Downloading input data...\n",
      "2022-09-27 14:19:09 Training - Downloading the training image......\n",
      "2022-09-27 14:20:15 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:21 INFO 140081561163584] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:21 INFO 140081561163584] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '5', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'epochs': '20', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_layers': '3', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:21 INFO 140081561163584] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.05', 'early_stopping_patience': '10', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'gaussian', 'mini_batch_size': '32', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '3', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '5', 'epochs': '20', 'prediction_length': '5', 'time_freq': 'D'}\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:21 INFO 140081561163584] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] random_seed is None\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Training set statistics:\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Real time series\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] number of time series: 76\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] number of observations: 1140\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] mean target length: 15.0\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] min/mean/max target: 217.46400451660156/9868.291105999862/61318.95703125\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] mean abs(target): 9868.291105999862\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Small number of time series. Doing 5 passes over dataset with prob 0.8421052631578947 per epoch.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Test set statistics:\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Real time series\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] number of time series: 76\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] number of observations: 1520\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] mean target length: 20.0\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] min/mean/max target: 217.46400451660156/11219.35587993421/61318.95703125\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] mean abs(target): 11219.35587993421\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] contains missing values: no\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #memory_usage::<batchbuffer> = 0.147705078125 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] nvidia-smi: took 0.029 seconds to run.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288422.1748285, \"EndTime\": 1664288422.218981, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 41.21971130371094, \"count\": 1, \"min\": 41.21971130371094, \"max\": 41.21971130371094}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #memory_usage::<model> = 3 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288422.2191, \"EndTime\": 1664288422.2895956, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 114.60304260253906, \"count\": 1, \"min\": 114.60304260253906, \"max\": 114.60304260253906}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[0] Batch[0] avg_epoch_loss=8.533584\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=8.533583641052246\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[0] Batch[5] avg_epoch_loss=8.037916\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=8.037915627161661\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[0] Batch [5]#011Speed: 1799.55 samples/sec#011loss=8.037916\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[0] Batch[10] avg_epoch_loss=8.219445\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=8.437280750274658\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[0] Batch [10]#011Speed: 1603.01 samples/sec#011loss=8.437281\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] processed a total of 369 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288422.289698, \"EndTime\": 1664288422.8225496, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 532.7413082122803, \"count\": 1, \"min\": 532.7413082122803, \"max\": 532.7413082122803}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=692.4548311051595 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #quality_metric: host=algo-1, epoch=0, train loss <loss>=8.343199888865152\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_f76f2509-6674-4b54-8514-b9c0071b2290-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288422.8226457, \"EndTime\": 1664288422.8381696, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.884233474731445, \"count\": 1, \"min\": 14.884233474731445, \"max\": 14.884233474731445}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] Epoch[1] Batch[0] avg_epoch_loss=8.104437\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:22 INFO 140081561163584] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=8.104436874389648\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[1] Batch[5] avg_epoch_loss=7.700526\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=7.700525522232056\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[1] Batch [5]#011Speed: 2011.56 samples/sec#011loss=7.700526\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[1] Batch[10] avg_epoch_loss=7.945761\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=8.240042686462402\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[1] Batch [10]#011Speed: 1901.75 samples/sec#011loss=8.240043\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] processed a total of 333 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288422.8382351, \"EndTime\": 1664288423.1337538, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 295.44901847839355, \"count\": 1, \"min\": 295.44901847839355, \"max\": 295.44901847839355}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1126.6061638531826 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=1, train loss <loss>=7.945760596882213\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_34c70202-a589-48f9-8617-7bd6367973b3-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288423.1338418, \"EndTime\": 1664288423.1443574, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.936094284057617, \"count\": 1, \"min\": 9.936094284057617, \"max\": 9.936094284057617}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[2] Batch[0] avg_epoch_loss=7.948760\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=7.948760032653809\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[2] Batch[5] avg_epoch_loss=7.728195\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=7.728194713592529\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[2] Batch [5]#011Speed: 1831.05 samples/sec#011loss=7.728195\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[2] Batch[10] avg_epoch_loss=7.740914\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=7.756176280975342\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[2] Batch [10]#011Speed: 1695.14 samples/sec#011loss=7.756176\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] processed a total of 363 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288423.1444254, \"EndTime\": 1664288423.4762766, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 331.7832946777344, \"count\": 1, \"min\": 331.7832946777344, \"max\": 331.7832946777344}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1093.6538194105242 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=2, train loss <loss>=7.972810784975688\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[3] Batch[0] avg_epoch_loss=7.926622\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=7.92662239074707\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[3] Batch[5] avg_epoch_loss=7.588345\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=7.588344653447469\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[3] Batch [5]#011Speed: 2158.84 samples/sec#011loss=7.588345\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[3] Batch[10] avg_epoch_loss=7.705916\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=3, batch=10 train loss <loss>=7.847001743316651\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[3] Batch [10]#011Speed: 1931.55 samples/sec#011loss=7.847002\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] processed a total of 356 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288423.4763665, \"EndTime\": 1664288423.7680411, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 291.11719131469727, \"count\": 1, \"min\": 291.11719131469727, \"max\": 291.11719131469727}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1222.3106324666544 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=3, train loss <loss>=7.8644859790802\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_48d9e08e-0d75-4bf4-bdcf-d0bdba54d956-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288423.7681282, \"EndTime\": 1664288423.7803607, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.629343032836914, \"count\": 1, \"min\": 11.629343032836914, \"max\": 11.629343032836914}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[4] Batch[0] avg_epoch_loss=7.406165\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=7.40616512298584\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[4] Batch[5] avg_epoch_loss=7.483195\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=7.483195225397746\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:23 INFO 140081561163584] Epoch[4] Batch [5]#011Speed: 2109.94 samples/sec#011loss=7.483195\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[4] Batch[10] avg_epoch_loss=7.513805\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=7.5505365371704105\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[4] Batch [10]#011Speed: 1590.62 samples/sec#011loss=7.550537\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] processed a total of 348 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288423.780428, \"EndTime\": 1664288424.0610828, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 280.5917263031006, \"count\": 1, \"min\": 280.5917263031006, \"max\": 280.5917263031006}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1239.7368283814394 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=4, train loss <loss>=7.513804912567139\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_dc398eb3-5867-46fd-b9c3-6d779462c3d0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288424.0611591, \"EndTime\": 1664288424.0762134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 14.41335678100586, \"count\": 1, \"min\": 14.41335678100586, \"max\": 14.41335678100586}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[5] Batch[0] avg_epoch_loss=7.391823\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=7.391822814941406\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[5] Batch[5] avg_epoch_loss=7.547908\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=7.547908067703247\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[5] Batch [5]#011Speed: 1326.92 samples/sec#011loss=7.547908\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[5] Batch[10] avg_epoch_loss=7.581398\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=7.621586704254151\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[5] Batch [10]#011Speed: 1098.99 samples/sec#011loss=7.621587\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] processed a total of 356 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288424.0762763, \"EndTime\": 1664288424.5253296, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 448.9855766296387, \"count\": 1, \"min\": 448.9855766296387, \"max\": 448.9855766296387}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=792.0497687248037 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=5, train loss <loss>=7.7609860102335615\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[6] Batch[0] avg_epoch_loss=7.659071\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=7.65907096862793\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[6] Batch[5] avg_epoch_loss=7.706379\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=7.706379334131877\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[6] Batch [5]#011Speed: 1302.09 samples/sec#011loss=7.706379\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[6] Batch[10] avg_epoch_loss=7.584403\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=7.438031578063965\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] Epoch[6] Batch [10]#011Speed: 1465.37 samples/sec#011loss=7.438032\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] processed a total of 336 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288424.5257647, \"EndTime\": 1664288424.919797, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 392.51184463500977, \"count\": 1, \"min\": 392.51184463500977, \"max\": 392.51184463500977}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=855.694552961535 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] #quality_metric: host=algo-1, epoch=6, train loss <loss>=7.584403081373735\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:24 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[7] Batch[0] avg_epoch_loss=7.481031\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=7.4810309410095215\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[7] Batch[5] avg_epoch_loss=7.357239\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=7.357239484786987\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[7] Batch [5]#011Speed: 1407.03 samples/sec#011loss=7.357239\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[7] Batch[10] avg_epoch_loss=7.640519\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=7.980453968048096\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[7] Batch [10]#011Speed: 1448.98 samples/sec#011loss=7.980454\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] processed a total of 347 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288424.919905, \"EndTime\": 1664288425.2851238, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 364.774227142334, \"count\": 1, \"min\": 364.774227142334, \"max\": 364.774227142334}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=950.8712398431747 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=7, train loss <loss>=7.640518795360219\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[8] Batch[0] avg_epoch_loss=7.564896\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=7.564896106719971\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[8] Batch[5] avg_epoch_loss=7.258334\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=7.258333683013916\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[8] Batch [5]#011Speed: 1336.39 samples/sec#011loss=7.258334\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[8] Batch[10] avg_epoch_loss=7.662584\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=8.14768362045288\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[8] Batch [10]#011Speed: 1250.66 samples/sec#011loss=8.147684\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] processed a total of 350 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288425.2852347, \"EndTime\": 1664288425.7042358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 418.5214042663574, \"count\": 1, \"min\": 418.5214042663574, \"max\": 418.5214042663574}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=835.5439293274071 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=8, train loss <loss>=7.662583654577082\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[9] Batch[0] avg_epoch_loss=7.236781\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=7.236780643463135\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[9] Batch[5] avg_epoch_loss=7.491224\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=7.491223891576131\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:25 INFO 140081561163584] Epoch[9] Batch [5]#011Speed: 1476.23 samples/sec#011loss=7.491224\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[9] Batch[10] avg_epoch_loss=7.525107\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=9, batch=10 train loss <loss>=7.565767383575439\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[9] Batch [10]#011Speed: 1198.38 samples/sec#011loss=7.565767\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] processed a total of 354 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288425.7045562, \"EndTime\": 1664288426.1144624, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.3015193939209, \"count\": 1, \"min\": 409.3015193939209, \"max\": 409.3015193939209}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=864.6055286916176 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=9, train loss <loss>=7.710650444030762\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[10] Batch[0] avg_epoch_loss=7.844555\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=7.844555377960205\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[10] Batch[5] avg_epoch_loss=7.445893\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=7.445892810821533\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[10] Batch [5]#011Speed: 1239.51 samples/sec#011loss=7.445893\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[10] Batch[10] avg_epoch_loss=7.452642\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=7.460740756988526\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[10] Batch [10]#011Speed: 1278.30 samples/sec#011loss=7.460741\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] processed a total of 326 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288426.114551, \"EndTime\": 1664288426.5228915, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.82952308654785, \"count\": 1, \"min\": 407.82952308654785, \"max\": 407.82952308654785}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=799.0070162910952 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=10, train loss <loss>=7.452641877261075\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_860043aa-984b-4e15-8210-42f9122e433f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288426.5230212, \"EndTime\": 1664288426.5336897, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.074377059936523, \"count\": 1, \"min\": 10.074377059936523, \"max\": 10.074377059936523}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[11] Batch[0] avg_epoch_loss=6.810351\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=6.8103508949279785\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[11] Batch[5] avg_epoch_loss=7.190742\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=7.190741856892903\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[11] Batch [5]#011Speed: 1626.72 samples/sec#011loss=7.190742\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[11] Batch[10] avg_epoch_loss=7.400307\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=7.651785469055175\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Epoch[11] Batch [10]#011Speed: 1413.51 samples/sec#011loss=7.651785\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] processed a total of 346 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288426.533771, \"EndTime\": 1664288426.9093108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 375.469446182251, \"count\": 1, \"min\": 375.469446182251, \"max\": 375.469446182251}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=921.1866666751301 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] #quality_metric: host=algo-1, epoch=11, train loss <loss>=7.4003071351484815\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:26 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_171956b8-925d-4a54-83e1-7dea1f27758b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288426.9094012, \"EndTime\": 1664288426.9210997, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.124372482299805, \"count\": 1, \"min\": 11.124372482299805, \"max\": 11.124372482299805}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[12] Batch[0] avg_epoch_loss=6.952956\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=6.952955722808838\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[12] Batch[5] avg_epoch_loss=7.324253\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=7.324252605438232\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[12] Batch [5]#011Speed: 1871.32 samples/sec#011loss=7.324253\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] processed a total of 308 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288426.9211655, \"EndTime\": 1664288427.1929464, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 271.6817855834961, \"count\": 1, \"min\": 271.6817855834961, \"max\": 271.6817855834961}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1133.1581628714782 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=12, train loss <loss>=7.4634232997894285\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[13] Batch[0] avg_epoch_loss=7.371941\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=7.371941089630127\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[13] Batch[5] avg_epoch_loss=7.327420\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=7.327419598897298\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[13] Batch [5]#011Speed: 1787.28 samples/sec#011loss=7.327420\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[13] Batch[10] avg_epoch_loss=7.307530\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=7.283662509918213\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[13] Batch [10]#011Speed: 1670.73 samples/sec#011loss=7.283663\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] processed a total of 346 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288427.1930299, \"EndTime\": 1664288427.5084534, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 314.87178802490234, \"count\": 1, \"min\": 314.87178802490234, \"max\": 314.87178802490234}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1098.0825436988264 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=13, train loss <loss>=7.307530012997714\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_42d1af28-5c50-4b90-8404-298ef97ba051-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288427.508631, \"EndTime\": 1664288427.5213375, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.829614639282227, \"count\": 1, \"min\": 11.829614639282227, \"max\": 11.829614639282227}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[14] Batch[0] avg_epoch_loss=7.638272\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=7.638271808624268\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[14] Batch[5] avg_epoch_loss=7.050087\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=7.050086975097656\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[14] Batch [5]#011Speed: 1981.50 samples/sec#011loss=7.050087\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[14] Batch[10] avg_epoch_loss=7.230009\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=7.445915412902832\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[14] Batch [10]#011Speed: 1558.61 samples/sec#011loss=7.445915\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] processed a total of 373 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288427.5214016, \"EndTime\": 1664288427.8687847, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 347.31483459472656, \"count\": 1, \"min\": 347.31483459472656, \"max\": 347.31483459472656}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1073.5364418003032 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=14, train loss <loss>=7.373362461725871\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] Epoch[15] Batch[0] avg_epoch_loss=7.178004\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:27 INFO 140081561163584] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=7.178004264831543\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[15] Batch[5] avg_epoch_loss=7.164964\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=7.164963801701863\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[15] Batch [5]#011Speed: 2190.08 samples/sec#011loss=7.164964\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[15] Batch[10] avg_epoch_loss=7.334919\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=7.538866138458252\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[15] Batch [10]#011Speed: 1615.54 samples/sec#011loss=7.538866\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] processed a total of 339 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288427.8688776, \"EndTime\": 1664288428.1715405, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 301.9840717315674, \"count\": 1, \"min\": 301.9840717315674, \"max\": 301.9840717315674}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1121.8432872562014 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=15, train loss <loss>=7.334919409318403\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[16] Batch[0] avg_epoch_loss=7.260864\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=7.2608642578125\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[16] Batch[5] avg_epoch_loss=7.215688\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=7.215687990188599\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[16] Batch [5]#011Speed: 2005.03 samples/sec#011loss=7.215688\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[16] Batch[10] avg_epoch_loss=7.324391\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=7.45483512878418\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[16] Batch [10]#011Speed: 1655.94 samples/sec#011loss=7.454835\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] processed a total of 388 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288428.1716976, \"EndTime\": 1664288428.4973938, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 324.9804973602295, \"count\": 1, \"min\": 324.9804973602295, \"max\": 324.9804973602295}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1193.3636078316345 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=16, train loss <loss>=7.208859920501709\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_9b484477-81b9-4889-8af7-7db988560082-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288428.4975002, \"EndTime\": 1664288428.5078835, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.824514389038086, \"count\": 1, \"min\": 9.824514389038086, \"max\": 9.824514389038086}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[17] Batch[0] avg_epoch_loss=7.025792\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=7.025792121887207\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[17] Batch[5] avg_epoch_loss=6.883849\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=6.883848667144775\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[17] Batch [5]#011Speed: 1844.15 samples/sec#011loss=6.883849\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[17] Batch[10] avg_epoch_loss=7.057875\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=7.266705894470215\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[17] Batch [10]#011Speed: 1665.27 samples/sec#011loss=7.266706\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] processed a total of 366 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288428.5079527, \"EndTime\": 1664288428.8268795, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 318.8595771789551, \"count\": 1, \"min\": 318.8595771789551, \"max\": 318.8595771789551}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1147.3791373552617 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=17, train loss <loss>=7.068396250406901\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/state_602b15ad-1c34-4ec4-8b21-90c89d204b06-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288428.8269672, \"EndTime\": 1664288428.8384504, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.875940322875977, \"count\": 1, \"min\": 10.875940322875977, \"max\": 10.875940322875977}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] Epoch[18] Batch[0] avg_epoch_loss=6.825311\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:28 INFO 140081561163584] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=6.825311183929443\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[18] Batch[5] avg_epoch_loss=7.007122\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=7.007121880849202\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[18] Batch [5]#011Speed: 1935.78 samples/sec#011loss=7.007122\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[18] Batch[10] avg_epoch_loss=7.129835\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=7.277089691162109\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[18] Batch [10]#011Speed: 1367.48 samples/sec#011loss=7.277090\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] processed a total of 370 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288428.838521, \"EndTime\": 1664288429.1698737, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 331.2807083129883, \"count\": 1, \"min\": 331.2807083129883, \"max\": 331.2807083129883}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=1115.377303230434 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=18, train loss <loss>=7.069904685020447\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[19] Batch[0] avg_epoch_loss=7.664556\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=7.66455602645874\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[19] Batch[5] avg_epoch_loss=6.954733\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=6.954732656478882\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[19] Batch [5]#011Speed: 1324.69 samples/sec#011loss=6.954733\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[19] Batch[10] avg_epoch_loss=7.125511\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=7.330444622039795\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Epoch[19] Batch [10]#011Speed: 1064.89 samples/sec#011loss=7.330445\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] processed a total of 369 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.1702702, \"EndTime\": 1664288429.6356242, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.7533893585205, \"count\": 1, \"min\": 464.7533893585205, \"max\": 464.7533893585205}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #throughput_metric: host=algo-1, train throughput=793.1361769838037 records/second\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, epoch=19, train loss <loss>=7.208591063817342\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] loss did not improve\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Final loss: 7.068396250406901 (occurred at epoch 17)\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #quality_metric: host=algo-1, train final_loss <loss>=7.068396250406901\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 WARNING 140081561163584] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.636064, \"EndTime\": 1664288429.709984, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 70.7859992980957, \"count\": 1, \"min\": 70.7859992980957, \"max\": 70.7859992980957}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.710437, \"EndTime\": 1664288429.7473097, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 108.1540584564209, \"count\": 1, \"min\": 108.1540584564209, \"max\": 108.1540584564209}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.7476957, \"EndTime\": 1664288429.7526803, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 4.933357238769531, \"count\": 1, \"min\": 4.933357238769531, \"max\": 4.933357238769531}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] #memory_usage::<batchbuffer> = 0.147705078125 mb\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:29 INFO 140081561163584] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.7530146, \"EndTime\": 1664288429.7570999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.049591064453125, \"count\": 1, \"min\": 0.049591064453125, \"max\": 0.049591064453125}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288429.7575176, \"EndTime\": 1664288431.025134, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 1268.0864334106445, \"count\": 1, \"min\": 1268.0864334106445, \"max\": 1268.0864334106445}}}\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, RMSE): 11977.366879058905\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, mean_absolute_QuantileLoss): 1881922.3995157878\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, mean_wQuantileLoss): 0.32426984145717713\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.1]): 0.16544988165238078\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.2]): 0.26610742938216664\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.3]): 0.3444619633831156\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.4]): 0.3956782288059882\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.5]): 0.41959810557139593\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.6]): 0.42286854446083233\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.7]): 0.38767200162891724\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.8]): 0.31652498777474725\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #test_score (algo-1, wQuantileLoss[0.9]): 0.20006743045504993\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #quality_metric: host=algo-1, test RMSE <loss>=11977.366879058905\u001b[0m\n",
      "\u001b[34m[09/27/2022 14:20:31 INFO 140081561163584] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.32426984145717713\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664288431.0252566, \"EndTime\": 1664288431.0316145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 14.249563217163086, \"count\": 1, \"min\": 14.249563217163086, \"max\": 14.249563217163086}, \"totaltime\": {\"sum\": 9348.36721420288, \"count\": 1, \"min\": 9348.36721420288, \"max\": 9348.36721420288}}}\u001b[0m\n",
      "\n",
      "2022-09-27 14:20:40 Uploading - Uploading generated training model\n",
      "2022-09-27 14:21:09 Completed - Training job completed\n",
      "Training seconds: 133\n",
      "Billable seconds: 133\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "data_channels = {\"train\": f\"s3://{s3_data_path}/train/\", \"test\": f\"s3://{s3_data_path}/test/\"}\n",
    "\n",
    "job_name = f'jumpstart-example-deepar-{strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())}'\n",
    "estimator.fit(inputs=data_channels, job_name = job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3a66273",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:21:28.815397Z",
     "iopub.status.busy": "2022-09-27T14:21:28.814828Z",
     "iopub.status.idle": "2022-09-27T14:25:00.790463Z",
     "shell.execute_reply": "2022-09-27T14:25:00.789980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------!"
     ]
    }
   ],
   "source": [
    "job_name = estimator.latest_training_job.name\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    image_uri=image_uri,\n",
    "    role=role,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1a4d676",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:25:00.796837Z",
     "iopub.status.busy": "2022-09-27T14:25:00.796114Z",
     "iopub.status.idle": "2022-09-27T14:25:00.799989Z",
     "shell.execute_reply": "2022-09-27T14:25:00.799497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumpstart-example-deepar-2022-09-27-14-16-44'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8ec5b98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:25:00.804328Z",
     "iopub.status.busy": "2022-09-27T14:25:00.803518Z",
     "iopub.status.idle": "2022-09-27T14:25:00.806277Z",
     "shell.execute_reply": "2022-09-27T14:25:00.805759Z"
    }
   },
   "outputs": [],
   "source": [
    "dic={\"epn\":endpoint_name}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46377804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:25:00.810362Z",
     "iopub.status.busy": "2022-09-27T14:25:00.809732Z",
     "iopub.status.idle": "2022-09-27T14:25:00.812929Z",
     "shell.execute_reply": "2022-09-27T14:25:00.812254Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('epn.pkl', 'wb') as file:\n",
    "    pickle.dump(dic, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9e3e5d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-27T14:25:00.819036Z",
     "iopub.status.busy": "2022-09-27T14:25:00.817007Z",
     "iopub.status.idle": "2022-09-27T14:25:00.879311Z",
     "shell.execute_reply": "2022-09-27T14:25:00.878719Z"
    }
   },
   "outputs": [],
   "source": [
    "s3.upload_file(\"epn.pkl\", bucket, prefix + \"/data/output/\" + \"epn.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1691ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
